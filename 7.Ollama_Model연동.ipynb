{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1109fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0482fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Alright, so I'm trying to understand what LangChain is. From the previous \"\n",
      " 'explanation, it seems like a tool for integrating language models into '\n",
      " \"machine learning workflows. But now I'm wondering if there's more to it or \"\n",
      " 'how exactly it works.\\n'\n",
      " '\\n'\n",
      " 'First off, I know that in machine learning, we often use datasets and models '\n",
      " 'to train systems. So maybe LangChain provides some extra functionality '\n",
      " 'beyond just using existing libraries. It might help with processing these '\n",
      " 'datasets further before feeding them into the model.\\n'\n",
      " '\\n'\n",
      " \"I've heard of NLP tasks like text classification or sentiment analysis. Does \"\n",
      " \"LangChain handle those? I'm not sure how it would integrate into a typical \"\n",
      " 'workflow. Would you need to use external libraries for that, or does it '\n",
      " 'simplify things by handling some of the preprocessing?\\n'\n",
      " '\\n'\n",
      " 'Another thing is that maybe LangChain allows for more flexible model '\n",
      " 'configurations. Like, instead of just using a single model, could one '\n",
      " 'experiment with different architectures? Or perhaps adjust hyperparameters '\n",
      " \"in a way that's integrated into the Langchain framework.\\n\"\n",
      " '\\n'\n",
      " \"I'm also thinking about data pipelines. In machine learning workflows, it's \"\n",
      " 'common to have steps like data loading, cleaning, and transforming data '\n",
      " 'before feeding it into models. Does LangChain offer some of these steps as '\n",
      " 'part of its integration process, making it easier for someone to set up a '\n",
      " 'complete pipeline without having to write extensive code from scratch?\\n'\n",
      " '\\n'\n",
      " 'Moreover, I recall that sometimes tools or libraries provide specific '\n",
      " 'datasets for certain tasks. If LangChain is the right tool for integrating '\n",
      " 'language models, maybe it could provide access to new or specialized '\n",
      " \"datasets that aren't available elsewhere, which would be beneficial for \"\n",
      " 'researchers and developers.\\n'\n",
      " '\\n'\n",
      " \"I'm also curious about how accurate and efficient it is compared to other \"\n",
      " \"methods. Since it's a language model-based system, might there be \"\n",
      " 'limitations in accuracy due to the nature of the data? Or are there ways to '\n",
      " 'mitigate those issues?\\n'\n",
      " '\\n'\n",
      " 'Another aspect to consider is community support and resources. If someone '\n",
      " 'uses LangChain for their work, does they have access to tutorials or '\n",
      " 'documentation to help them get started? That could make it more '\n",
      " 'user-friendly compared to using other tools that might require more setup '\n",
      " 'from the beginning.\\n'\n",
      " '\\n'\n",
      " \"I should also think about potential downsides. Maybe there's a learning \"\n",
      " \"curve involved if someone isn't familiar with integrating language models \"\n",
      " 'with machine learning workflows. Or perhaps the integration is done in a way '\n",
      " 'that makes things more complicated than necessary, requiring users to '\n",
      " 'implement certain steps themselves.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain seems like a tool that enhances the process of using '\n",
      " 'language models by providing pre-processed data, flexible model '\n",
      " \"configurations, and possibly new datasets. However, I'm not entirely sure \"\n",
      " 'how all these elements work together within a typical machine learning '\n",
      " 'workflow or if there are potential limitations to consider.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a versatile tool designed to simplify the integration of '\n",
      " \"language models into machine learning workflows. Here's a structured \"\n",
      " 'overview of its key features and considerations:\\n'\n",
      " '\\n'\n",
      " '1. **Integration with Language Models**: LangChain leverages existing '\n",
      " 'libraries, offering a seamless transition from data processing to model '\n",
      " 'training.\\n'\n",
      " '\\n'\n",
      " '2. **Data Handling**: It provides functions for preprocessing datasets, '\n",
      " 'which can streamline tasks like text classification or sentiment analysis by '\n",
      " 'reducing the need for extensive external tools.\\n'\n",
      " '\\n'\n",
      " '3. **Model Experimentation**: LangChain allows experimentation with various '\n",
      " 'language models and hyperparameters, facilitating deeper understanding '\n",
      " 'through iterative adjustments.\\n'\n",
      " '\\n'\n",
      " '4. **Comprehensive Data Pipelines**: The tool facilitates a well-rounded '\n",
      " 'data pipeline, encompassing loading, cleaning, transforming, and '\n",
      " 'preprocessing steps essential for robust model training.\\n'\n",
      " '\\n'\n",
      " '5. **Specialized Datasets**: It offers access to new datasets tailored for '\n",
      " 'specific tasks, enhancing the reach of language models beyond standard '\n",
      " 'datasets.\\n'\n",
      " '\\n'\n",
      " '6. **Accuracy Considerations**: While leveraging the power of language '\n",
      " 'models, potential limitations in dataset quality may affect accuracy, though '\n",
      " 'tools exist to mitigate these issues through careful data preparation.\\n'\n",
      " '\\n'\n",
      " '7. **Community Support and Resources**: Tutorials and documentation can be '\n",
      " 'invaluable for new users, ensuring a user-friendly experience compared to '\n",
      " 'more complex integration methods.\\n'\n",
      " '\\n'\n",
      " '8. **Learning Curve**: While beneficial, the need to set up certain steps '\n",
      " 'might require some initial learning, potentially deterring those unfamiliar '\n",
      " 'with integrating language models into workflows.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, LangChain enhances machine learning workflows by providing '\n",
      " 'pre-processed data and model configurations, but its effectiveness may '\n",
      " 'depend on factors such as dataset quality and user familiarity.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cf396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"파이썬은 무엇인가요?\" which translates to \"What is Python?\" in English. I need to provide a clear and accurate answer.\n",
      "\n",
      "First, I should start by defining Python as a programming language. Mention that it's widely used for various purposes like web development, data analysis, artificial intelligence, and more. It's known for its simplicity and readability.\n",
      "\n",
      "I should also highlight its features, such as a large standard library, easy-to-learn syntax, and cross-platform compatibility. Maybe mention that it's open-source and has a vast community. \n",
      "\n",
      "It's important to note that Python is not just for beginners; it's also used by professionals. Examples of popular frameworks like Django and Flask can illustrate its applications. \n",
      "\n",
      "I should avoid technical jargon and keep the explanation straightforward. Make sure to explain why Python is popular, like its readability and versatility. Also, mention that it's used in both small scripts and large-scale projects.\n",
      "\n",
      "Check for any misconceptions. For instance, while Python is easy to learn, it's not the only language for data analysis. Also, note that it's not just for data science but has many other uses. \n",
      "\n",
      "Finally, conclude by emphasizing Python's role in the tech industry and its importance in various fields. Keep the answer concise but comprehensive.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어로, 간결하고 읽기 쉽게 설계된 언어로, 다양한 분야에서 널리 사용됩니다. 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. **간결한 문법**:  \n",
      "   파이썬은 '명확한 구문'을 통해 코드를 작성할 수 있어, 초반 배우기 쉬운 점이 특징입니다. 예:  \n",
      "   ```python\n",
      "   print(\"Hello, World!\")\n",
      "   ```\n",
      "\n",
      "2. **다양한 활용 분야**:  \n",
      "   - **웹 개발**: Django, Flask 등 프레임워크 사용  \n",
      "   - **데이터 분석**: Pandas, NumPy, Matplotlib 등 라이브러리 활용  \n",
      "   - **AI/머신러닝**: TensorFlow, PyTorch 등 라이브러리 사용  \n",
      "   - **데이터 과학**: Scikit-learn, Keras 등 도구  \n",
      "   - **제작**: GUI 앱( PyQt, Tkinter), 템플릿 마크업( Jinja2) 등  \n",
      "\n",
      "3. **크로스플랫폼 호환성**:  \n",
      "   Windows, macOS, Linux 등 다양한 운영 체제에서 동작 가능하며, 배포 시도 필요 없음.\n",
      "\n",
      "4. **크로스-프로젝트 지원**:  \n",
      "   대규모 커뮤니티와 오픈소스 기반으로, 빠르게 발전 중이며, 라이브러리(예: Flask, Django)가 풍부합니다.\n",
      "\n",
      "5. **보안 강화**:  \n",
      "   웹 앱 개발 시 안정적인 구조로, 인증/인가 과정에서 유용합니다.\n",
      "\n",
      "**특징 요약**:  \n",
      "- **편리한 문법** + **다양한 활용** + **크로스플랫폼** + **대규모 커뮤니티**  \n",
      "- **초보자도 쉽게 배우고, 프로젝트부터 대규모 개발까지 활용 가능**\n",
      "\n",
      "파이썬은 기술 분야에서 빠르게 확장되고 있으며, 데이터 과학, AI, 웹 개발 등 다양한 분야에서 핵심 역할을 합니다. 🐍\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429e6740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger between 9.9 and 9.11, I'll start by comparing their whole number parts.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "Next, I'll look at the decimal parts. \n",
      "\n",
      "For 9.9, the decimal part is 0.9.\n",
      "For 9.11, the decimal part is 0.11.\n",
      "\n",
      "Since 0.9 is greater than 0.11, it means that 9.9 has a larger decimal value compared to 9.11.\n",
      "\n",
      "Therefore, 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Number Parts\n",
      "Both numbers have the same whole number part:\n",
      "- **9** (from **9.9**)  \n",
      "- **9** (from **9.11**)  \n",
      "\n",
      "Since they are equal, we need to move on to the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Decimal Parts\n",
      "- **0.9** (from **9.9**)  \n",
      "- **0.11** (from **9.11**)  \n",
      "\n",
      "To compare these decimals:\n",
      "1. **Convert them to the same number of decimal places**:  \n",
      "   - **0.9** can be written as **0.90**.\n",
      "2. Now, we have:\n",
      "   - **0.90**\n",
      "   - **0.11**\n",
      "\n",
      "3. Compare each corresponding digit from left to right:\n",
      "   - **Tenths place**: 9 vs. 1  \n",
      "     Since **9 > 1**, **0.90 > 0.11**.\n",
      "\n",
      "### Conclusion\n",
      "Since the decimal part of **9.9** is greater than that of **9.11**, we conclude that:\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
      "\\]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger between 9.9 and 9.11, I'll start by comparing their whole number parts.\n",
       "\n",
       "Both numbers have the same whole number part, which is 9.\n",
       "\n",
       "Next, I'll look at the decimal parts. \n",
       "\n",
       "For 9.9, the decimal part is 0.9.\n",
       "For 9.11, the decimal part is 0.11.\n",
       "\n",
       "Since 0.9 is greater than 0.11, it means that 9.9 has a larger decimal value compared to 9.11.\n",
       "\n",
       "Therefore, 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Number Parts\n",
       "Both numbers have the same whole number part:\n",
       "- **9** (from **9.9**)  \n",
       "- **9** (from **9.11**)  \n",
       "\n",
       "Since they are equal, we need to move on to the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Decimal Parts\n",
       "- **0.9** (from **9.9**)  \n",
       "- **0.11** (from **9.11**)  \n",
       "\n",
       "To compare these decimals:\n",
       "1. **Convert them to the same number of decimal places**:  \n",
       "   - **0.9** can be written as **0.90**.\n",
       "2. Now, we have:\n",
       "   - **0.90**\n",
       "   - **0.11**\n",
       "\n",
       "3. Compare each corresponding digit from left to right:\n",
       "   - **Tenths place**: 9 vs. 1  \n",
       "     Since **9 > 1**, **0.90 > 0.11**.\n",
       "\n",
       "### Conclusion\n",
       "Since the decimal part of **9.9** is greater than that of **9.11**, we conclude that:\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4591d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, I need to compare these two numbers.\n",
      "\n",
      "First, I know that 9.9 is a decimal number. Let me write them out: 9.9 and 9.11. Both have the same whole number part, which is 9. So the difference is in the decimal parts. \n",
      "\n",
      "The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. Wait, 0.9 is larger than 0.11, right? Because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is bigger than 0.11. \n",
      "\n",
      "But let me make sure I'm not making a mistake here. Sometimes when comparing decimals, you have to consider the place value. Let's break it down. \n",
      "\n",
      "For 9.9, the decimal part is 0.9, which can be written as 9/10. For 9.11, the decimal part is 0.11, which is 11/100. So 9/10 is 90/100, and 11/100 is 11/100. Since 90/100 is greater than 11/100, 9.9 is larger than 9.11. \n",
      "\n",
      "Another way to think about it is to subtract the two numbers. If I subtract 9.11 from 9.9, I get 9.9 - 9.11. Let's do the subtraction step by step. \n",
      "\n",
      "9.9 is the same as 9.90. So subtracting 9.11 from 9.90 would be:\n",
      "\n",
      "9.90\n",
      "-9.11\n",
      "------\n",
      "0.79\n",
      "\n",
      "So the result is 0.79, which is positive. That means 9.9 is larger than 9.11. \n",
      "\n",
      "I can also think about the positions of the decimals. The first number has two decimal places, and the second has two as well. But the first number's decimal part is 0.9, which is more than 0.11. So even though both have two decimal places, the first number's decimal part is larger. \n",
      "\n",
      "Wait, but 0.9 is the same as 0.90. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 and 9.11, the first number is larger because the tenths place is 9 vs. 1, and the hundredths place is 0 vs. 1. So even though the hundredths place is 0 in 9.90, the tenths place is 9, which is more than 1 in the tenths place of 9.11. \n",
      "\n",
      "So yeah, 9.9 is bigger than 9.11. I don't think I made any mistakes here. The key was to compare the decimal parts and realize that 0.9 is greater than 0.11.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "\n",
      "**이유:**  \n",
      "- 두 수 모두 9를 기준으로 시작합니다.  \n",
      "- 9.9의 소수 부분은 0.9(9/10)이고, 9.11의 소수 부분은 0.11(11/100)입니다.  \n",
      "- 0.9는 0.11보다 큽니다. 따라서 9.9 > 9.11입니다.  \n",
      "\n",
      "**결론:**  \n",
      "**9.9**이 더 큰 수입니다."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, I need to compare these two numbers.\n",
       "\n",
       "First, I know that 9.9 is a decimal number. Let me write them out: 9.9 and 9.11. Both have the same whole number part, which is 9. So the difference is in the decimal parts. \n",
       "\n",
       "The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. Wait, 0.9 is larger than 0.11, right? Because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is bigger than 0.11. \n",
       "\n",
       "But let me make sure I'm not making a mistake here. Sometimes when comparing decimals, you have to consider the place value. Let's break it down. \n",
       "\n",
       "For 9.9, the decimal part is 0.9, which can be written as 9/10. For 9.11, the decimal part is 0.11, which is 11/100. So 9/10 is 90/100, and 11/100 is 11/100. Since 90/100 is greater than 11/100, 9.9 is larger than 9.11. \n",
       "\n",
       "Another way to think about it is to subtract the two numbers. If I subtract 9.11 from 9.9, I get 9.9 - 9.11. Let's do the subtraction step by step. \n",
       "\n",
       "9.9 is the same as 9.90. So subtracting 9.11 from 9.90 would be:\n",
       "\n",
       "9.90\n",
       "-9.11\n",
       "------\n",
       "0.79\n",
       "\n",
       "So the result is 0.79, which is positive. That means 9.9 is larger than 9.11. \n",
       "\n",
       "I can also think about the positions of the decimals. The first number has two decimal places, and the second has two as well. But the first number's decimal part is 0.9, which is more than 0.11. So even though both have two decimal places, the first number's decimal part is larger. \n",
       "\n",
       "Wait, but 0.9 is the same as 0.90. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 and 9.11, the first number is larger because the tenths place is 9 vs. 1, and the hundredths place is 0 vs. 1. So even though the hundredths place is 0 in 9.90, the tenths place is 9, which is more than 1 in the tenths place of 9.11. \n",
       "\n",
       "So yeah, 9.9 is bigger than 9.11. I don't think I made any mistakes here. The key was to compare the decimal parts and realize that 0.9 is greater than 0.11.\n",
       "</think>\n",
       "\n",
       "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
       "\n",
       "**이유:**  \n",
       "- 두 수 모두 9를 기준으로 시작합니다.  \n",
       "- 9.9의 소수 부분은 0.9(9/10)이고, 9.11의 소수 부분은 0.11(11/100)입니다.  \n",
       "- 0.9는 0.11보다 큽니다. 따라서 9.9 > 9.11입니다.  \n",
       "\n",
       "**결론:**  \n",
       "**9.9**이 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca6340",
   "metadata": {},
   "source": [
    "* LangGraph 를 사용하여 두개의 모델 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0dad669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.7\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 추론 모델\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "# 응답 모델 (한글처리 가능)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.7)\n",
    "print(generation_model)\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f236ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure out the answer. \n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, breaking it down, after the decimal, the first number has a 9 in the tenths place, and the second has a 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "Wait, but I should make sure there's no confusion. Sometimes people might think that 9.9 is shorter than 9.11, but actually, when you add a zero, it's still the same value. So 9.90 is equal to 9.9, which is definitely bigger than 9.11. \n",
      "\n",
      "I think that's it. The key is to align the decimals and compare each digit step by step. The answer should be that 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "두 수는 정수 부분(9)이 같으므로 소수 부분을 비교해야 합니다.  \n",
      "9.9는 9.90으로 쓰면 9.90과 9.11을 비교할 수 있습니다.  \n",
      "소수점 첫째 자리(9 vs 1)에서는 9가 더 큘이므로, 9.90은 9.11보다 더 큰 수입니다.  \n",
      "따라서 **9.9**이 더 큰 수입니다.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nTo make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\\n\\nNow that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\\n\\nStarting with the units place: Both numbers have a 9 in the units place, so they are equal there.\\n\\nNext, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure out the answer. \\n\\nFirst, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \\n\\nSo, breaking it down, after the decimal, the first number has a 9 in the tenths place, and the second has a 1. Since 9 is greater than 1, the first number is larger. \\n\\nWait, but I should make sure there's no confusion. Sometimes people might think that 9.9 is shorter than 9.11, but actually, when you add a zero, it's still the same value. So 9.90 is equal to 9.9, which is definitely bigger than 9.11. \\n\\nI think that's it. The key is to align the decimals and compare each digit step by step. The answer should be that 9.9 is larger than 9.11.\\n</think>\\n\\n9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \\n두 수는 정수 부분(9)이 같으므로 소수 부분을 비교해야 합니다.  \\n9.9는 9.90으로 쓰면 9.90과 9.11을 비교할 수 있습니다.  \\n소수점 첫째 자리(9 vs 1)에서는 9가 더 큘이므로, 9.90은 9.11보다 더 큰 수입니다.  \\n따라서 **9.9**이 더 큰 수입니다.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure out the answer. \n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, breaking it down, after the decimal, the first number has a 9 in the tenths place, and the second has a 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "Wait, but I should make sure there's no confusion. Sometimes people might think that 9.9 is shorter than 9.11, but actually, when you add a zero, it's still the same value. So 9.90 is equal to 9.9, which is definitely bigger than 9.11. \n",
      "\n",
      "I think that's it. The key is to align the decimals and compare each digit step by step. The answer should be that 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "두 수는 정수 부분(9)이 같으므로 소수 부분을 비교해야 합니다.  \n",
      "9.9는 9.90으로 쓰면 9.90과 9.11을 비교할 수 있습니다.  \n",
      "소수점 첫째 자리(9 vs 1)에서는 9가 더 큘이므로, 9.90은 9.11보다 더 큰 수입니다.  \n",
      "따라서 **9.9**이 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980bcaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c6ee15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\n",
      "\n",
      "Now that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\n",
      "\n",
      "Starting with the units place: Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking whether 9.9 is larger than 9.11. Hmm, both numbers are decimals. I remember that when comparing decimals, you start from the leftmost digit. \n",
      "\n",
      "First, I notice that both numbers have the same integer part, which is 9. So, that's equal so far. Now, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when adding a zero. The second number is 9.11. \n",
      "\n",
      "So, if I write them both with two decimal places, 9.90 vs. 9.11. Comparing the tenths place: 9 versus 1. Since 9 is greater than 1, the first number is larger. Therefore, 9.9 is greater than 9.11. \n",
      "\n",
      "Wait, but the user provided an inference that was empty. I need to make sure I didn't miss anything. The key here is aligning the decimal places. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11. So the conclusion is correct. \n",
      "\n",
      "I should present this in a clear, conversational way without using markdown. Make sure to explain each step and the reasoning clearly. Also, check if there's any ambiguity, but in this case, the numbers are straightforward. The answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 9.9입니다. 두 수는 모두 9를 기준으로 시작하지만, 소수 부분에서 비교해야 합니다. 9.9는 9.90과 같으며, 9.11보다 9의 자리에서 더 큰 수를 가집니다. 따라서 9.9는 9.11보다 더 큰 수입니다.<think>\n",
      "Okay, let's tackle this question. The user is asking whether 9.9 is larger than 9.11. Hmm, both numbers are decimals. I remember that when comparing decimals, you start from the leftmost digit. \n",
      "\n",
      "First, I notice that both numbers have the same integer part, which is 9. So, that's equal so far. Now, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when adding a zero. The second number is 9.11. \n",
      "\n",
      "So, if I write them both with two decimal places, 9.90 vs. 9.11. Comparing the tenths place: 9 versus 1. Since 9 is greater than 1, the first number is larger. Therefore, 9.9 is greater than 9.11. \n",
      "\n",
      "Wait, but the user provided an inference that was empty. I need to make sure I didn't miss anything. The key here is aligning the decimal places. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11. So the conclusion is correct. \n",
      "\n",
      "I should present this in a clear, conversational way without using markdown. Make sure to explain each step and the reasoning clearly. Also, check if there's any ambiguity, but in this case, the numbers are straightforward. The answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 9.9입니다. 두 수는 모두 9를 기준으로 시작하지만, 소수 부분에서 비교해야 합니다. 9.9는 9.90과 같으며, 9.11보다 9의 자리에서 더 큰 수를 가집니다. 따라서 9.9는 9.11보다 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bd109",
   "metadata": {},
   "source": [
    "### Gradio 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce94f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-AGfTwH54-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# # Gradio 인터페이스 설정\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    # launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-AGfTwH54-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
