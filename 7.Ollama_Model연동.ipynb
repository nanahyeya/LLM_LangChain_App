{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1109fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0482fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Alright, so I'm trying to understand what LangChain is. From the previous \"\n",
      " 'explanation, it seems like a tool for integrating language models into '\n",
      " \"machine learning workflows. But now I'm wondering if there's more to it or \"\n",
      " 'how exactly it works.\\n'\n",
      " '\\n'\n",
      " 'First off, I know that in machine learning, we often use datasets and models '\n",
      " 'to train systems. So maybe LangChain provides some extra functionality '\n",
      " 'beyond just using existing libraries. It might help with processing these '\n",
      " 'datasets further before feeding them into the model.\\n'\n",
      " '\\n'\n",
      " \"I've heard of NLP tasks like text classification or sentiment analysis. Does \"\n",
      " \"LangChain handle those? I'm not sure how it would integrate into a typical \"\n",
      " 'workflow. Would you need to use external libraries for that, or does it '\n",
      " 'simplify things by handling some of the preprocessing?\\n'\n",
      " '\\n'\n",
      " 'Another thing is that maybe LangChain allows for more flexible model '\n",
      " 'configurations. Like, instead of just using a single model, could one '\n",
      " 'experiment with different architectures? Or perhaps adjust hyperparameters '\n",
      " \"in a way that's integrated into the Langchain framework.\\n\"\n",
      " '\\n'\n",
      " \"I'm also thinking about data pipelines. In machine learning workflows, it's \"\n",
      " 'common to have steps like data loading, cleaning, and transforming data '\n",
      " 'before feeding it into models. Does LangChain offer some of these steps as '\n",
      " 'part of its integration process, making it easier for someone to set up a '\n",
      " 'complete pipeline without having to write extensive code from scratch?\\n'\n",
      " '\\n'\n",
      " 'Moreover, I recall that sometimes tools or libraries provide specific '\n",
      " 'datasets for certain tasks. If LangChain is the right tool for integrating '\n",
      " 'language models, maybe it could provide access to new or specialized '\n",
      " \"datasets that aren't available elsewhere, which would be beneficial for \"\n",
      " 'researchers and developers.\\n'\n",
      " '\\n'\n",
      " \"I'm also curious about how accurate and efficient it is compared to other \"\n",
      " \"methods. Since it's a language model-based system, might there be \"\n",
      " 'limitations in accuracy due to the nature of the data? Or are there ways to '\n",
      " 'mitigate those issues?\\n'\n",
      " '\\n'\n",
      " 'Another aspect to consider is community support and resources. If someone '\n",
      " 'uses LangChain for their work, does they have access to tutorials or '\n",
      " 'documentation to help them get started? That could make it more '\n",
      " 'user-friendly compared to using other tools that might require more setup '\n",
      " 'from the beginning.\\n'\n",
      " '\\n'\n",
      " \"I should also think about potential downsides. Maybe there's a learning \"\n",
      " \"curve involved if someone isn't familiar with integrating language models \"\n",
      " 'with machine learning workflows. Or perhaps the integration is done in a way '\n",
      " 'that makes things more complicated than necessary, requiring users to '\n",
      " 'implement certain steps themselves.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain seems like a tool that enhances the process of using '\n",
      " 'language models by providing pre-processed data, flexible model '\n",
      " \"configurations, and possibly new datasets. However, I'm not entirely sure \"\n",
      " 'how all these elements work together within a typical machine learning '\n",
      " 'workflow or if there are potential limitations to consider.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a versatile tool designed to simplify the integration of '\n",
      " \"language models into machine learning workflows. Here's a structured \"\n",
      " 'overview of its key features and considerations:\\n'\n",
      " '\\n'\n",
      " '1. **Integration with Language Models**: LangChain leverages existing '\n",
      " 'libraries, offering a seamless transition from data processing to model '\n",
      " 'training.\\n'\n",
      " '\\n'\n",
      " '2. **Data Handling**: It provides functions for preprocessing datasets, '\n",
      " 'which can streamline tasks like text classification or sentiment analysis by '\n",
      " 'reducing the need for extensive external tools.\\n'\n",
      " '\\n'\n",
      " '3. **Model Experimentation**: LangChain allows experimentation with various '\n",
      " 'language models and hyperparameters, facilitating deeper understanding '\n",
      " 'through iterative adjustments.\\n'\n",
      " '\\n'\n",
      " '4. **Comprehensive Data Pipelines**: The tool facilitates a well-rounded '\n",
      " 'data pipeline, encompassing loading, cleaning, transforming, and '\n",
      " 'preprocessing steps essential for robust model training.\\n'\n",
      " '\\n'\n",
      " '5. **Specialized Datasets**: It offers access to new datasets tailored for '\n",
      " 'specific tasks, enhancing the reach of language models beyond standard '\n",
      " 'datasets.\\n'\n",
      " '\\n'\n",
      " '6. **Accuracy Considerations**: While leveraging the power of language '\n",
      " 'models, potential limitations in dataset quality may affect accuracy, though '\n",
      " 'tools exist to mitigate these issues through careful data preparation.\\n'\n",
      " '\\n'\n",
      " '7. **Community Support and Resources**: Tutorials and documentation can be '\n",
      " 'invaluable for new users, ensuring a user-friendly experience compared to '\n",
      " 'more complex integration methods.\\n'\n",
      " '\\n'\n",
      " '8. **Learning Curve**: While beneficial, the need to set up certain steps '\n",
      " 'might require some initial learning, potentially deterring those unfamiliar '\n",
      " 'with integrating language models into workflows.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, LangChain enhances machine learning workflows by providing '\n",
      " 'pre-processed data and model configurations, but its effectiveness may '\n",
      " 'depend on factors such as dataset quality and user familiarity.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ llama3.2 ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cf396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”?\" which translates to \"What is Python?\" in English. I need to provide a clear and accurate answer.\n",
      "\n",
      "First, I should start by defining Python as a programming language. Mention that it's widely used for various purposes like web development, data analysis, artificial intelligence, and more. It's known for its simplicity and readability.\n",
      "\n",
      "I should also highlight its features, such as a large standard library, easy-to-learn syntax, and cross-platform compatibility. Maybe mention that it's open-source and has a vast community. \n",
      "\n",
      "It's important to note that Python is not just for beginners; it's also used by professionals. Examples of popular frameworks like Django and Flask can illustrate its applications. \n",
      "\n",
      "I should avoid technical jargon and keep the explanation straightforward. Make sure to explain why Python is popular, like its readability and versatility. Also, mention that it's used in both small scripts and large-scale projects.\n",
      "\n",
      "Check for any misconceptions. For instance, while Python is easy to learn, it's not the only language for data analysis. Also, note that it's not just for data science but has many other uses. \n",
      "\n",
      "Finally, conclude by emphasizing Python's role in the tech industry and its importance in various fields. Keep the answer concise but comprehensive.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ ì„¤ê³„ëœ ì–¸ì–´ë¡œ, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ê°„ê²°í•œ ë¬¸ë²•**:  \n",
      "   íŒŒì´ì¬ì€ 'ëª…í™•í•œ êµ¬ë¬¸'ì„ í†µí•´ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì–´, ì´ˆë°˜ ë°°ìš°ê¸° ì‰¬ìš´ ì ì´ íŠ¹ì§•ì…ë‹ˆë‹¤. ì˜ˆ:  \n",
      "   ```python\n",
      "   print(\"Hello, World!\")\n",
      "   ```\n",
      "\n",
      "2. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**:  \n",
      "   - **ì›¹ ê°œë°œ**: Django, Flask ë“± í”„ë ˆì„ì›Œí¬ ì‚¬ìš©  \n",
      "   - **ë°ì´í„° ë¶„ì„**: Pandas, NumPy, Matplotlib ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©  \n",
      "   - **AI/ë¨¸ì‹ ëŸ¬ë‹**: TensorFlow, PyTorch ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©  \n",
      "   - **ë°ì´í„° ê³¼í•™**: Scikit-learn, Keras ë“± ë„êµ¬  \n",
      "   - **ì œì‘**: GUI ì•±( PyQt, Tkinter), í…œí”Œë¦¿ ë§ˆí¬ì—…( Jinja2) ë“±  \n",
      "\n",
      "3. **í¬ë¡œìŠ¤í”Œë«í¼ í˜¸í™˜ì„±**:  \n",
      "   Windows, macOS, Linux ë“± ë‹¤ì–‘í•œ ìš´ì˜ ì²´ì œì—ì„œ ë™ì‘ ê°€ëŠ¥í•˜ë©°, ë°°í¬ ì‹œë„ í•„ìš” ì—†ìŒ.\n",
      "\n",
      "4. **í¬ë¡œìŠ¤-í”„ë¡œì íŠ¸ ì§€ì›**:  \n",
      "   ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ì™€ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ, ë¹ ë¥´ê²Œ ë°œì „ ì¤‘ì´ë©°, ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: Flask, Django)ê°€ í’ë¶€í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ë³´ì•ˆ ê°•í™”**:  \n",
      "   ì›¹ ì•± ê°œë°œ ì‹œ ì•ˆì •ì ì¸ êµ¬ì¡°ë¡œ, ì¸ì¦/ì¸ê°€ ê³¼ì •ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "**íŠ¹ì§• ìš”ì•½**:  \n",
      "- **í¸ë¦¬í•œ ë¬¸ë²•** + **ë‹¤ì–‘í•œ í™œìš©** + **í¬ë¡œìŠ¤í”Œë«í¼** + **ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°**  \n",
      "- **ì´ˆë³´ìë„ ì‰½ê²Œ ë°°ìš°ê³ , í”„ë¡œì íŠ¸ë¶€í„° ëŒ€ê·œëª¨ ê°œë°œê¹Œì§€ í™œìš© ê°€ëŠ¥**\n",
      "\n",
      "íŒŒì´ì¬ì€ ê¸°ìˆ  ë¶„ì•¼ì—ì„œ ë¹ ë¥´ê²Œ í™•ì¥ë˜ê³  ìˆìœ¼ë©°, ë°ì´í„° ê³¼í•™, AI, ì›¹ ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤. ğŸ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ llama3.2 ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-AGfTwH54-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
