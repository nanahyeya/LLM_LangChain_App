{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1109fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0482fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Alright, so I'm trying to understand what LangChain is. From the previous \"\n",
      " 'explanation, it seems like a tool for integrating language models into '\n",
      " \"machine learning workflows. But now I'm wondering if there's more to it or \"\n",
      " 'how exactly it works.\\n'\n",
      " '\\n'\n",
      " 'First off, I know that in machine learning, we often use datasets and models '\n",
      " 'to train systems. So maybe LangChain provides some extra functionality '\n",
      " 'beyond just using existing libraries. It might help with processing these '\n",
      " 'datasets further before feeding them into the model.\\n'\n",
      " '\\n'\n",
      " \"I've heard of NLP tasks like text classification or sentiment analysis. Does \"\n",
      " \"LangChain handle those? I'm not sure how it would integrate into a typical \"\n",
      " 'workflow. Would you need to use external libraries for that, or does it '\n",
      " 'simplify things by handling some of the preprocessing?\\n'\n",
      " '\\n'\n",
      " 'Another thing is that maybe LangChain allows for more flexible model '\n",
      " 'configurations. Like, instead of just using a single model, could one '\n",
      " 'experiment with different architectures? Or perhaps adjust hyperparameters '\n",
      " \"in a way that's integrated into the Langchain framework.\\n\"\n",
      " '\\n'\n",
      " \"I'm also thinking about data pipelines. In machine learning workflows, it's \"\n",
      " 'common to have steps like data loading, cleaning, and transforming data '\n",
      " 'before feeding it into models. Does LangChain offer some of these steps as '\n",
      " 'part of its integration process, making it easier for someone to set up a '\n",
      " 'complete pipeline without having to write extensive code from scratch?\\n'\n",
      " '\\n'\n",
      " 'Moreover, I recall that sometimes tools or libraries provide specific '\n",
      " 'datasets for certain tasks. If LangChain is the right tool for integrating '\n",
      " 'language models, maybe it could provide access to new or specialized '\n",
      " \"datasets that aren't available elsewhere, which would be beneficial for \"\n",
      " 'researchers and developers.\\n'\n",
      " '\\n'\n",
      " \"I'm also curious about how accurate and efficient it is compared to other \"\n",
      " \"methods. Since it's a language model-based system, might there be \"\n",
      " 'limitations in accuracy due to the nature of the data? Or are there ways to '\n",
      " 'mitigate those issues?\\n'\n",
      " '\\n'\n",
      " 'Another aspect to consider is community support and resources. If someone '\n",
      " 'uses LangChain for their work, does they have access to tutorials or '\n",
      " 'documentation to help them get started? That could make it more '\n",
      " 'user-friendly compared to using other tools that might require more setup '\n",
      " 'from the beginning.\\n'\n",
      " '\\n'\n",
      " \"I should also think about potential downsides. Maybe there's a learning \"\n",
      " \"curve involved if someone isn't familiar with integrating language models \"\n",
      " 'with machine learning workflows. Or perhaps the integration is done in a way '\n",
      " 'that makes things more complicated than necessary, requiring users to '\n",
      " 'implement certain steps themselves.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain seems like a tool that enhances the process of using '\n",
      " 'language models by providing pre-processed data, flexible model '\n",
      " \"configurations, and possibly new datasets. However, I'm not entirely sure \"\n",
      " 'how all these elements work together within a typical machine learning '\n",
      " 'workflow or if there are potential limitations to consider.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a versatile tool designed to simplify the integration of '\n",
      " \"language models into machine learning workflows. Here's a structured \"\n",
      " 'overview of its key features and considerations:\\n'\n",
      " '\\n'\n",
      " '1. **Integration with Language Models**: LangChain leverages existing '\n",
      " 'libraries, offering a seamless transition from data processing to model '\n",
      " 'training.\\n'\n",
      " '\\n'\n",
      " '2. **Data Handling**: It provides functions for preprocessing datasets, '\n",
      " 'which can streamline tasks like text classification or sentiment analysis by '\n",
      " 'reducing the need for extensive external tools.\\n'\n",
      " '\\n'\n",
      " '3. **Model Experimentation**: LangChain allows experimentation with various '\n",
      " 'language models and hyperparameters, facilitating deeper understanding '\n",
      " 'through iterative adjustments.\\n'\n",
      " '\\n'\n",
      " '4. **Comprehensive Data Pipelines**: The tool facilitates a well-rounded '\n",
      " 'data pipeline, encompassing loading, cleaning, transforming, and '\n",
      " 'preprocessing steps essential for robust model training.\\n'\n",
      " '\\n'\n",
      " '5. **Specialized Datasets**: It offers access to new datasets tailored for '\n",
      " 'specific tasks, enhancing the reach of language models beyond standard '\n",
      " 'datasets.\\n'\n",
      " '\\n'\n",
      " '6. **Accuracy Considerations**: While leveraging the power of language '\n",
      " 'models, potential limitations in dataset quality may affect accuracy, though '\n",
      " 'tools exist to mitigate these issues through careful data preparation.\\n'\n",
      " '\\n'\n",
      " '7. **Community Support and Resources**: Tutorials and documentation can be '\n",
      " 'invaluable for new users, ensuring a user-friendly experience compared to '\n",
      " 'more complex integration methods.\\n'\n",
      " '\\n'\n",
      " '8. **Learning Curve**: While beneficial, the need to set up certain steps '\n",
      " 'might require some initial learning, potentially deterring those unfamiliar '\n",
      " 'with integrating language models into workflows.\\n'\n",
      " '\\n'\n",
      " 'In conclusion, LangChain enhances machine learning workflows by providing '\n",
      " 'pre-processed data and model configurations, but its effectiveness may '\n",
      " 'depend on factors such as dataset quality and user familiarity.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ llama3.2 ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cf396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”?\" which translates to \"What is Python?\" in English. I need to provide a clear and accurate answer.\n",
      "\n",
      "First, I should start by defining Python as a programming language. Mention that it's widely used for various purposes like web development, data analysis, artificial intelligence, and more. It's known for its simplicity and readability.\n",
      "\n",
      "I should also highlight its features, such as a large standard library, easy-to-learn syntax, and cross-platform compatibility. Maybe mention that it's open-source and has a vast community. \n",
      "\n",
      "It's important to note that Python is not just for beginners; it's also used by professionals. Examples of popular frameworks like Django and Flask can illustrate its applications. \n",
      "\n",
      "I should avoid technical jargon and keep the explanation straightforward. Make sure to explain why Python is popular, like its readability and versatility. Also, mention that it's used in both small scripts and large-scale projects.\n",
      "\n",
      "Check for any misconceptions. For instance, while Python is easy to learn, it's not the only language for data analysis. Also, note that it's not just for data science but has many other uses. \n",
      "\n",
      "Finally, conclude by emphasizing Python's role in the tech industry and its importance in various fields. Keep the answer concise but comprehensive.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ ì„¤ê³„ëœ ì–¸ì–´ë¡œ, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ê°„ê²°í•œ ë¬¸ë²•**:  \n",
      "   íŒŒì´ì¬ì€ 'ëª…í™•í•œ êµ¬ë¬¸'ì„ í†µí•´ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì–´, ì´ˆë°˜ ë°°ìš°ê¸° ì‰¬ìš´ ì ì´ íŠ¹ì§•ì…ë‹ˆë‹¤. ì˜ˆ:  \n",
      "   ```python\n",
      "   print(\"Hello, World!\")\n",
      "   ```\n",
      "\n",
      "2. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**:  \n",
      "   - **ì›¹ ê°œë°œ**: Django, Flask ë“± í”„ë ˆì„ì›Œí¬ ì‚¬ìš©  \n",
      "   - **ë°ì´í„° ë¶„ì„**: Pandas, NumPy, Matplotlib ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©  \n",
      "   - **AI/ë¨¸ì‹ ëŸ¬ë‹**: TensorFlow, PyTorch ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©  \n",
      "   - **ë°ì´í„° ê³¼í•™**: Scikit-learn, Keras ë“± ë„êµ¬  \n",
      "   - **ì œì‘**: GUI ì•±( PyQt, Tkinter), í…œí”Œë¦¿ ë§ˆí¬ì—…( Jinja2) ë“±  \n",
      "\n",
      "3. **í¬ë¡œìŠ¤í”Œë«í¼ í˜¸í™˜ì„±**:  \n",
      "   Windows, macOS, Linux ë“± ë‹¤ì–‘í•œ ìš´ì˜ ì²´ì œì—ì„œ ë™ì‘ ê°€ëŠ¥í•˜ë©°, ë°°í¬ ì‹œë„ í•„ìš” ì—†ìŒ.\n",
      "\n",
      "4. **í¬ë¡œìŠ¤-í”„ë¡œì íŠ¸ ì§€ì›**:  \n",
      "   ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ì™€ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ, ë¹ ë¥´ê²Œ ë°œì „ ì¤‘ì´ë©°, ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: Flask, Django)ê°€ í’ë¶€í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ë³´ì•ˆ ê°•í™”**:  \n",
      "   ì›¹ ì•± ê°œë°œ ì‹œ ì•ˆì •ì ì¸ êµ¬ì¡°ë¡œ, ì¸ì¦/ì¸ê°€ ê³¼ì •ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "**íŠ¹ì§• ìš”ì•½**:  \n",
      "- **í¸ë¦¬í•œ ë¬¸ë²•** + **ë‹¤ì–‘í•œ í™œìš©** + **í¬ë¡œìŠ¤í”Œë«í¼** + **ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°**  \n",
      "- **ì´ˆë³´ìë„ ì‰½ê²Œ ë°°ìš°ê³ , í”„ë¡œì íŠ¸ë¶€í„° ëŒ€ê·œëª¨ ê°œë°œê¹Œì§€ í™œìš© ê°€ëŠ¥**\n",
      "\n",
      "íŒŒì´ì¬ì€ ê¸°ìˆ  ë¶„ì•¼ì—ì„œ ë¹ ë¥´ê²Œ í™•ì¥ë˜ê³  ìˆìœ¼ë©°, ë°ì´í„° ê³¼í•™, AI, ì›¹ ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤. ğŸ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ llama3.2 ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429e6740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger between 9.9 and 9.11, I'll start by comparing their whole number parts.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "Next, I'll look at the decimal parts. \n",
      "\n",
      "For 9.9, the decimal part is 0.9.\n",
      "For 9.11, the decimal part is 0.11.\n",
      "\n",
      "Since 0.9 is greater than 0.11, it means that 9.9 has a larger decimal value compared to 9.11.\n",
      "\n",
      "Therefore, 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Number Parts\n",
      "Both numbers have the same whole number part:\n",
      "- **9** (from **9.9**)  \n",
      "- **9** (from **9.11**)  \n",
      "\n",
      "Since they are equal, we need to move on to the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Decimal Parts\n",
      "- **0.9** (from **9.9**)  \n",
      "- **0.11** (from **9.11**)  \n",
      "\n",
      "To compare these decimals:\n",
      "1. **Convert them to the same number of decimal places**:  \n",
      "   - **0.9** can be written as **0.90**.\n",
      "2. Now, we have:\n",
      "   - **0.90**\n",
      "   - **0.11**\n",
      "\n",
      "3. Compare each corresponding digit from left to right:\n",
      "   - **Tenths place**: 9 vs. 1  \n",
      "     Since **9 > 1**, **0.90 > 0.11**.\n",
      "\n",
      "### Conclusion\n",
      "Since the decimal part of **9.9** is greater than that of **9.11**, we conclude that:\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
      "\\]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger between 9.9 and 9.11, I'll start by comparing their whole number parts.\n",
       "\n",
       "Both numbers have the same whole number part, which is 9.\n",
       "\n",
       "Next, I'll look at the decimal parts. \n",
       "\n",
       "For 9.9, the decimal part is 0.9.\n",
       "For 9.11, the decimal part is 0.11.\n",
       "\n",
       "Since 0.9 is greater than 0.11, it means that 9.9 has a larger decimal value compared to 9.11.\n",
       "\n",
       "Therefore, 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Number Parts\n",
       "Both numbers have the same whole number part:\n",
       "- **9** (from **9.9**)  \n",
       "- **9** (from **9.11**)  \n",
       "\n",
       "Since they are equal, we need to move on to the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Decimal Parts\n",
       "- **0.9** (from **9.9**)  \n",
       "- **0.11** (from **9.11**)  \n",
       "\n",
       "To compare these decimals:\n",
       "1. **Convert them to the same number of decimal places**:  \n",
       "   - **0.9** can be written as **0.90**.\n",
       "2. Now, we have:\n",
       "   - **0.90**\n",
       "   - **0.11**\n",
       "\n",
       "3. Compare each corresponding digit from left to right:\n",
       "   - **Tenths place**: 9 vs. 1  \n",
       "     Since **9 > 1**, **0.90 > 0.11**.\n",
       "\n",
       "### Conclusion\n",
       "Since the decimal part of **9.9** is greater than that of **9.11**, we conclude that:\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4591d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, I need to compare these two numbers.\n",
      "\n",
      "First, I know that 9.9 is a decimal number. Let me write them out: 9.9 and 9.11. Both have the same whole number part, which is 9. So the difference is in the decimal parts. \n",
      "\n",
      "The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. Wait, 0.9 is larger than 0.11, right? Because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is bigger than 0.11. \n",
      "\n",
      "But let me make sure I'm not making a mistake here. Sometimes when comparing decimals, you have to consider the place value. Let's break it down. \n",
      "\n",
      "For 9.9, the decimal part is 0.9, which can be written as 9/10. For 9.11, the decimal part is 0.11, which is 11/100. So 9/10 is 90/100, and 11/100 is 11/100. Since 90/100 is greater than 11/100, 9.9 is larger than 9.11. \n",
      "\n",
      "Another way to think about it is to subtract the two numbers. If I subtract 9.11 from 9.9, I get 9.9 - 9.11. Let's do the subtraction step by step. \n",
      "\n",
      "9.9 is the same as 9.90. So subtracting 9.11 from 9.90 would be:\n",
      "\n",
      "9.90\n",
      "-9.11\n",
      "------\n",
      "0.79\n",
      "\n",
      "So the result is 0.79, which is positive. That means 9.9 is larger than 9.11. \n",
      "\n",
      "I can also think about the positions of the decimals. The first number has two decimal places, and the second has two as well. But the first number's decimal part is 0.9, which is more than 0.11. So even though both have two decimal places, the first number's decimal part is larger. \n",
      "\n",
      "Wait, but 0.9 is the same as 0.90. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 and 9.11, the first number is larger because the tenths place is 9 vs. 1, and the hundredths place is 0 vs. 1. So even though the hundredths place is 0 in 9.90, the tenths place is 9, which is more than 1 in the tenths place of 9.11. \n",
      "\n",
      "So yeah, 9.9 is bigger than 9.11. I don't think I made any mistakes here. The key was to compare the decimal parts and realize that 0.9 is greater than 0.11.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ì´ìœ :**  \n",
      "- ë‘ ìˆ˜ ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.  \n",
      "- 9.9ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.9(9/10)ì´ê³ , 9.11ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.11(11/100)ì…ë‹ˆë‹¤.  \n",
      "- 0.9ëŠ” 0.11ë³´ë‹¤ í½ë‹ˆë‹¤. ë”°ë¼ì„œ 9.9 > 9.11ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ê²°ë¡ :**  \n",
      "**9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's see. The question is asking which is bigger between 9.9 and 9.11. Hmm, I need to compare these two numbers.\n",
       "\n",
       "First, I know that 9.9 is a decimal number. Let me write them out: 9.9 and 9.11. Both have the same whole number part, which is 9. So the difference is in the decimal parts. \n",
       "\n",
       "The first number, 9.9, has a decimal part of 0.9. The second number, 9.11, has a decimal part of 0.11. Wait, 0.9 is larger than 0.11, right? Because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is bigger than 0.11. \n",
       "\n",
       "But let me make sure I'm not making a mistake here. Sometimes when comparing decimals, you have to consider the place value. Let's break it down. \n",
       "\n",
       "For 9.9, the decimal part is 0.9, which can be written as 9/10. For 9.11, the decimal part is 0.11, which is 11/100. So 9/10 is 90/100, and 11/100 is 11/100. Since 90/100 is greater than 11/100, 9.9 is larger than 9.11. \n",
       "\n",
       "Another way to think about it is to subtract the two numbers. If I subtract 9.11 from 9.9, I get 9.9 - 9.11. Let's do the subtraction step by step. \n",
       "\n",
       "9.9 is the same as 9.90. So subtracting 9.11 from 9.90 would be:\n",
       "\n",
       "9.90\n",
       "-9.11\n",
       "------\n",
       "0.79\n",
       "\n",
       "So the result is 0.79, which is positive. That means 9.9 is larger than 9.11. \n",
       "\n",
       "I can also think about the positions of the decimals. The first number has two decimal places, and the second has two as well. But the first number's decimal part is 0.9, which is more than 0.11. So even though both have two decimal places, the first number's decimal part is larger. \n",
       "\n",
       "Wait, but 0.9 is the same as 0.90. So 9.9 is 9.90, and 9.11 is 9.11. Comparing 9.90 and 9.11, the first number is larger because the tenths place is 9 vs. 1, and the hundredths place is 0 vs. 1. So even though the hundredths place is 0 in 9.90, the tenths place is 9, which is more than 1 in the tenths place of 9.11. \n",
       "\n",
       "So yeah, 9.9 is bigger than 9.11. I don't think I made any mistakes here. The key was to compare the decimal parts and realize that 0.9 is greater than 0.11.\n",
       "</think>\n",
       "\n",
       "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
       "\n",
       "**ì´ìœ :**  \n",
       "- ë‘ ìˆ˜ ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.  \n",
       "- 9.9ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.9(9/10)ì´ê³ , 9.11ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.11(11/100)ì…ë‹ˆë‹¤.  \n",
       "- 0.9ëŠ” 0.11ë³´ë‹¤ í½ë‹ˆë‹¤. ë”°ë¼ì„œ 9.9 > 9.11ì…ë‹ˆë‹¤.  \n",
       "\n",
       "**ê²°ë¡ :**  \n",
       "**9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca6340",
   "metadata": {},
   "source": [
    "* LangGraph ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ê°œì˜ ëª¨ë¸ ì—°ë™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0dad669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.7\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\\n\\n        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\\n        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\\n        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\\n        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n        ì§€ì¹¨:\\n        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\\n        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\\n        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\\n        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\\n\\n        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        ì§ˆë¬¸: {question}\\n        ì¶”ë¡ : {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë¸\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "# ì‘ë‹µ ëª¨ë¸ (í•œê¸€ì²˜ë¦¬ ê°€ëŠ¥)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.7)\n",
    "print(generation_model)\n",
    "\n",
    "#LangGraphì—ì„œ State ì‚¬ìš©ìì •ì˜ í´ë˜ìŠ¤ëŠ” ë…¸ë“œ ê°„ì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í‹€ì…ë‹ˆë‹¤. \n",
    "#ë…¸ë“œ ê°„ì— ê³„ì† ì „ë‹¬í•˜ê³  ì‹¶ê±°ë‚˜, ê·¸ë˜í”„ ë‚´ì—ì„œ ìœ ì§€í•´ì•¼ í•  ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í™ë‹ˆë‹¤. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ì¶”ë¡ : {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f236ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure out the answer. \n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, breaking it down, after the decimal, the first number has a 9 in the tenths place, and the second has a 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "Wait, but I should make sure there's no confusion. Sometimes people might think that 9.9 is shorter than 9.11, but actually, when you add a zero, it's still the same value. So 9.90 is equal to 9.9, which is definitely bigger than 9.11. \n",
      "\n",
      "I think that's it. The key is to align the decimals and compare each digit step by step. The answer should be that 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ì“°ë©´ 9.90ê³¼ 9.11ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬(9 vs 1)ì—ì„œëŠ” 9ê°€ ë” í˜ì´ë¯€ë¡œ, 9.90ì€ 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "ë”°ë¼ì„œ **9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n",
      "{'question': '9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nTo make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\\n\\nNow that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\\n\\nStarting with the units place: Both numbers have a 9 in the units place, so they are equal there.\\n\\nNext, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure out the answer. \\n\\nFirst, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \\n\\nSo, breaking it down, after the decimal, the first number has a 9 in the tenths place, and the second has a 1. Since 9 is greater than 1, the first number is larger. \\n\\nWait, but I should make sure there's no confusion. Sometimes people might think that 9.9 is shorter than 9.11, but actually, when you add a zero, it's still the same value. So 9.90 is equal to 9.9, which is definitely bigger than 9.11. \\n\\nI think that's it. The key is to align the decimals and compare each digit step by step. The answer should be that 9.9 is larger than 9.11.\\n</think>\\n\\n9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \\në‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \\n9.9ëŠ” 9.90ìœ¼ë¡œ ì“°ë©´ 9.90ê³¼ 9.11ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \\nì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬(9 vs 1)ì—ì„œëŠ” 9ê°€ ë” í˜ì´ë¯€ë¡œ, 9.90ì€ 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \\në”°ë¼ì„œ **9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\"}\n",
      "==> ìƒì„±ëœ ë‹µë³€: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure out the answer. \n",
      "\n",
      "First, I remember that when comparing decimals, you start from the leftmost digit. Both numbers have the same integer part, which is 9. So that's equal. Then, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when written with two decimal places. The second number is 9.11. \n",
      "\n",
      "So, breaking it down, after the decimal, the first number has a 9 in the tenths place, and the second has a 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "Wait, but I should make sure there's no confusion. Sometimes people might think that 9.9 is shorter than 9.11, but actually, when you add a zero, it's still the same value. So 9.90 is equal to 9.9, which is definitely bigger than 9.11. \n",
      "\n",
      "I think that's it. The key is to align the decimals and compare each digit step by step. The answer should be that 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ˜ëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ìœ¼ë¡œ ì“°ë©´ 9.90ê³¼ 9.11ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬(9 vs 1)ì—ì„œëŠ” 9ê°€ ë” í˜ì´ë¯€ë¡œ, 9.90ì€ 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "ë”°ë¼ì„œ **9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "#DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwenë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°\n",
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "# invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ í˜¸ì¶œ\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"==> ìƒì„±ëœ ë‹µë³€: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980bcaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c6ee15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\n",
      "\n",
      "Now that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\n",
      "\n",
      "Starting with the units place: Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking whether 9.9 is larger than 9.11. Hmm, both numbers are decimals. I remember that when comparing decimals, you start from the leftmost digit. \n",
      "\n",
      "First, I notice that both numbers have the same integer part, which is 9. So, that's equal so far. Now, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when adding a zero. The second number is 9.11. \n",
      "\n",
      "So, if I write them both with two decimal places, 9.90 vs. 9.11. Comparing the tenths place: 9 versus 1. Since 9 is greater than 1, the first number is larger. Therefore, 9.9 is greater than 9.11. \n",
      "\n",
      "Wait, but the user provided an inference that was empty. I need to make sure I didn't miss anything. The key here is aligning the decimal places. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11. So the conclusion is correct. \n",
      "\n",
      "I should present this in a clear, conversational way without using markdown. Make sure to explain each step and the reasoning clearly. Also, check if there's any ambiguity, but in this case, the numbers are straightforward. The answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” 9.9ì…ë‹ˆë‹¤. ë‘ ìˆ˜ëŠ” ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ì§€ë§Œ, ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤. 9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ë³´ë‹¤ 9ì˜ ìë¦¬ì—ì„œ ë” í° ìˆ˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.<think>\n",
      "Okay, let's tackle this question. The user is asking whether 9.9 is larger than 9.11. Hmm, both numbers are decimals. I remember that when comparing decimals, you start from the leftmost digit. \n",
      "\n",
      "First, I notice that both numbers have the same integer part, which is 9. So, that's equal so far. Now, looking at the decimal parts. The first number is 9.9, which is the same as 9.90 when adding a zero. The second number is 9.11. \n",
      "\n",
      "So, if I write them both with two decimal places, 9.90 vs. 9.11. Comparing the tenths place: 9 versus 1. Since 9 is greater than 1, the first number is larger. Therefore, 9.9 is greater than 9.11. \n",
      "\n",
      "Wait, but the user provided an inference that was empty. I need to make sure I didn't miss anything. The key here is aligning the decimal places. Adding a zero to 9.9 makes it 9.90, which is clearly larger than 9.11. So the conclusion is correct. \n",
      "\n",
      "I should present this in a clear, conversational way without using markdown. Make sure to explain each step and the reasoning clearly. Also, check if there's any ambiguity, but in this case, the numbers are straightforward. The answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” 9.9ì…ë‹ˆë‹¤. ë‘ ìˆ˜ëŠ” ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ì§€ë§Œ, ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤. 9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ë³´ë‹¤ 9ì˜ ìë¦¬ì—ì„œ ë” í° ìˆ˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bd109",
   "metadata": {},
   "source": [
    "### Gradio ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce94f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-AGfTwH54-py3.12\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì • (Jupyter ë…¸íŠ¸ë¶ í˜¸í™˜)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter í™˜ê²½ì—ì„œëŠ” reconfigure ëŒ€ì‹  í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter ë…¸íŠ¸ë¶ì´ë‚˜ ë‹¤ë¥¸ í™˜ê²½ì—ì„œëŠ” íŒ¨ìŠ¤\n",
    "    pass\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ê³¼ ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰\n",
    "# - reasoning_model: ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë‚®ìŒ, ì •í™•í•œ ë¶„ì„ìš©)\n",
    "# - generation_model: ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë†’ìŒ, ì°½ì˜ì  ì‘ë‹µìš©)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ìƒíƒœ(State) ì •ì˜: ê·¸ë˜í”„ì—ì„œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ë°ì´í„° êµ¬ì¡°\n",
    "class State(TypedDict):\n",
    "    question: str   # ì‚¬ìš©ìì˜ ì§ˆë¬¸\n",
    "    thinking: str   # ì¶”ë¡  ê²°ê³¼\n",
    "    answer: str     # ìµœì¢… ë‹µë³€\n",
    "\n",
    "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ í•œêµ­ì–´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        \n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"ì§ˆë¬¸: {question}\n",
    "        \n",
    "        ì¶”ë¡  ê³¼ì •: {thinking}\n",
    "        \n",
    "        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"ë¬¸ìì—´ì´ UTF-8ë¡œ ì œëŒ€ë¡œ ì¸ì½”ë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³€í™˜\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # ë¬¸ìì—´ì´ì§€ë§Œ ì¸ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # ë¬¸ìì—´ì„ UTF-8ë¡œ ì¸ì½”ë”©í–ˆë‹¤ê°€ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì •ë¦¬\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] ì…ë ¥ ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] ì§ˆë¬¸ íƒ€ì…: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5ë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜: ìƒíƒœ(State) ê°„ì˜ íë¦„ì„ ì •ì˜\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# # Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "# def launch_gradio():\n",
    "#     iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "#     iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n",
    "    # launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-AGfTwH54-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
