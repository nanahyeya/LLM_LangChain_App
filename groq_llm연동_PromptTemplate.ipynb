{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9a673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "981e738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6aad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15336d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002489C36FA70> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002489C847EC0> root_client=<openai.OpenAI object at 0x000002489A10C2F0> root_async_client=<openai.AsyncOpenAI object at 0x000002489C455E20> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9097ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "<class 'str'>\n",
      "ì‘ë‹µ: íŒŒì´ì¬! í”„ë¡œê·¸ë˜ë° ì„¸ê³„ì—ì„œ ë§¤ìš° ì¸ê¸° ìˆëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. ì €ëŠ” ê°œë°œìë¡œì„œ íŒŒì´ì¬ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "**íŒŒì´ì¬ì´ë€?**\n",
      "\n",
      "íŒŒì´ì¬ì€ 1991ë…„ Guido van Rossumì— ì˜í•´ ê°œë°œëœ ê³ ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. íŒŒì´ì¬ì€ ë™ì  íƒ€ì´í•‘(dynamic typing) ì–¸ì–´ì´ë©°, ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°(OOP) íŒ¨ëŸ¬ë‹¤ì„ì„ ë”°ë¦…ë‹ˆë‹¤. íŒŒì´ì¬ì˜ ë””ìì¸ ì² í•™ì€ ì½”ë“œ ê°€ë…ì„±ê³¼ ê°„ê²°ì„±ì„ ê°•ì¡°í•˜ë©°, ê°œë°œìê°€ ë¹ ë¥´ê²Œ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
      "\n",
      "**íŒŒì´ì¬ì˜ íŠ¹ì§•**\n",
      "\n",
      "íŒŒì´ì¬ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ì‰¬ìš´ í•™ìŠµ ê³¡ì„ **: íŒŒì´ì¬ì€ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆì–´, í”„ë¡œê·¸ë˜ë°ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ” ì‚¬ëŒë“¤ë„ ì‰½ê²Œ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "2. **ê³ ìˆ˜ì¤€ ì–¸ì–´**: íŒŒì´ì¬ì€ í•˜ë“œì›¨ì–´ì™€ ìš´ì˜ ì²´ì œ ë ˆë²¨ì˜ ë³µì¡ì„±ì„ ì¶”ìƒí™”í•˜ì—¬, ê°œë°œìê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
      "3. **ë™ì  íƒ€ì´í•‘**: íŒŒì´ì¬ì€ ë³€ìˆ˜ì˜ íƒ€ì…ì„ ì„ ì–¸í•  í•„ìš”ê°€ ì—†ìœ¼ë©°, ì‹¤í–‰ ì‹œì— íƒ€ì…ì´ ê²°ì •ë©ë‹ˆë‹¤.\n",
      "4. **ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°**: íŒŒì´ì¬ì€ OOP íŒ¨ëŸ¬ë‹¤ì„ì„ ì§€ì›í•˜ë©°, í´ë˜ìŠ¤, ê°ì²´, ìƒì†, ë‹¤í˜•ì„± ë“±ì˜ ê°œë…ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
      "5. **ëŒ€ê·œëª¨ ë¼ì´ë¸ŒëŸ¬ë¦¬**: íŒŒì´ì¬ì€ ë°©ëŒ€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ëª¨ë“ˆì„ ë³´ìœ í•˜ê³  ìˆì–´, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**íŒŒì´ì¬ì˜ í™œìš© ë¶„ì•¼**\n",
      "\n",
      "íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ì›¹ ê°œë°œ**: íŒŒì´ì¬ì€ ì›¹ ê°œë°œì„ ìœ„í•´ Flask, Django ë“±ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "2. **ë°ì´í„° ê³¼í•™**: íŒŒì´ì¬ì€ ë°ì´í„° ê³¼í•™ì„ ìœ„í•´ NumPy, pandas, scikit-learn ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "3. **ì¸ê³µì§€ëŠ¥**: íŒŒì´ì¬ì€ ì¸ê³µì§€ëŠ¥ì„ ìœ„í•´ TensorFlow, Keras ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "4. **ìë™í™”**: íŒŒì´ì¬ì€ ìë™í™”ë¥¼ ìœ„í•´ Robot Framework ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "5. **êµìœ¡**: íŒŒì´ì¬ì€ êµìœ¡ ê¸°ê´€ì—ì„œ í”„ë¡œê·¸ë˜ë° ì…ë¬¸ ì–¸ì–´ë¡œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "**íŒŒì´ì¬ì˜ ë²„ì „**\n",
      "\n",
      "íŒŒì´ì¬ì€ ë‘ ê°€ì§€ ì£¼ìš” ë²„ì „ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n",
      "\n",
      "1. **íŒŒì´ì¬ 2.x**: 2000ë…„ì— ì¶œì‹œëœ íŒŒì´ì¬ 2.xëŠ” 2015ë…„ì— ì§€ì›ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "2. **íŒŒì´ì¬ 3.x**: 2008ë…„ì— ì¶œì‹œëœ íŒŒì´ì¬ 3.xëŠ” í˜„ì¬ ê°€ì¥ ìµœì‹ ì˜ ë²„ì „ì´ë©°, í™œë°œí•˜ê²Œ ì§€ì›ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**ê²°ë¡ **\n",
      "\n",
      "íŒŒì´ì¬ì€ ì‰½ê³  ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë©°, ê°œë°œìê°€ ë¹ ë¥´ê²Œ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. íŒŒì´ì¬ì€ êµìœ¡ ê¸°ê´€ì—ì„œ í”„ë¡œê·¸ë˜ë° ì…ë¬¸ ì–¸ì–´ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, ê°œë°œìë“¤ ì‚¬ì´ì—ì„œ ì¸ê¸°ê°€ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(type(response.content))\n",
    "    print(\"ì‘ë‹µ:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be6835",
   "metadata": {},
   "source": [
    "LCEL\n",
    "Prompt + LLMì„ Chainìœ¼ë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0e483ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\\n    ')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "    \"\"\")                                     \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918cd610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain ì—°ê²° (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a67e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCEL\n",
    "Prompt + LLM + OutputParserì„ Chainìœ¼ë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0277560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain ì—°ê²° (LCEL) prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa763d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# chain í˜¸ì¶œ\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e927295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChainì€ ë‹¤ì–‘í•œ ì œí’ˆê³¼ ë„êµ¬ë¥¼ ì œê³µí•˜ì—¬ ê°œë°œìì™€ ê¸°ì—…ì´ ì¸ê³µì§€ëŠ¥(AI) ê¸°ìˆ ì„ ë³´ë‹¤ ì‰½ê²Œ í†µí•©í•˜ê³  í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤. LangChainì˜ ì£¼ìš” ì œí’ˆ ì¤‘ LangSmithì™€ LangServeì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### LangSmith\n",
      "\n",
      "LangSmithëŠ” LangChainì—ì„œ ì œê³µí•˜ëŠ” í”Œë«í¼ìœ¼ë¡œ, ê°œë°œìê°€ ë­ì²´ì¸ ê¸°ë°˜ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë” ì‰½ê²Œ ê°œë°œ, í…ŒìŠ¤íŠ¸, ë°°í¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. LangSmithë¥¼ í†µí•´ ê°œë°œìëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "- **ê°œë°œ í™˜ê²½ ì œê³µ**: LangChainì˜ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë­ì²´ì¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆëŠ” í†µí•© í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "- **ì›Œí¬í”Œë¡œìš° ê´€ë¦¬**: ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬ ë° AI ì›Œí¬í”Œë¡œë¥¼ ì‰½ê²Œ ê´€ë¦¬í•˜ê³  ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ì—ì´ì „íŠ¸ ê´€ë¦¬**: ë‹¤ì–‘í•œ AI ì—ì´ì „íŠ¸ë¥¼ ê´€ë¦¬í•˜ê³ , ê·¸ë“¤ì˜ ìƒí˜¸ ì‘ìš©ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ë°ì´í„° ê´€ë¦¬**: AI ëª¨ë¸ì˜ í•™ìŠµê³¼ ì¶”ë¡ ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "LangSmithëŠ” ê°œë°œìê°€ AI ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê³  ìš´ì˜í•˜ëŠ” ë° í•„ìš”í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "### LangServe\n",
      "\n",
      "LangServeëŠ” LangChainì—ì„œ ì œê³µí•˜ëŠ” ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ì œí’ˆìœ¼ë¡œ, LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°°í¬ ë° ìš´ì˜ì„ ê°„ì†Œí™”í•©ë‹ˆë‹¤. LangServeë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ëˆ„ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "- **API ì œê³µ**: LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ RESTful APIë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•´ ì¤Œìœ¼ë¡œì¨, í´ë¼ì´ì–¸íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì„œë²„ì™€ í†µì‹ í•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "- **í™•ì¥ì„±**: ëŒ€ê·œëª¨ íŠ¸ë˜í”½ê³¼ ìš”ì²­ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì–´, ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í™•ì¥ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n",
      "- **ëª¨ë‹ˆí„°ë§ ë° ë¡œê·¸ ê´€ë¦¬**: ë°°í¬ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³ , ë¡œê·¸ë¥¼ ê´€ë¦¬í•˜ì—¬ ë¬¸ì œ í•´ê²°ì„ ë•ìŠµë‹ˆë‹¤.\n",
      "\n",
      "LangServeëŠ” LangChain ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ê¸°íƒ€ ì œí’ˆ\n",
      "\n",
      "LangChainì€ LangSmithì™€ LangServe ì™¸ì—ë„ ë‹¤ì–‘í•œ ë‹¤ë¥¸ ë„êµ¬ì™€ ì œí’ˆì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë“¤ ì œí’ˆì€ íŠ¹ì • ì‘ì—…ì´ë‚˜ ì‚°ì—…ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, ê°œë°œìì™€ ê¸°ì—…ì´ AI ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "- **LangChain SDK**: LangChainì˜ í•µì‹¬ ê¸°ëŠ¥ì— ì ‘ê·¼í•˜ê³ , ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ í‚¤íŠ¸ì…ë‹ˆë‹¤.\n",
      "- **LangChain ì»¤ë®¤ë‹ˆí‹°**: LangChain ì‚¬ìš©ìì™€ ê°œë°œìê°€ ì„œë¡œ ì†Œí†µí•˜ê³ , ì§€ì‹ê³¼ ê²½í—˜ì„ ê³µìœ í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ì œí’ˆê³¼ ë„êµ¬ë¥¼ í†µí•´ LangChainì€ ê°œë°œìì™€ ê¸°ì—…ì´ AI ê¸°ìˆ ì„ ë³´ë‹¤ ì‰½ê³  íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# chain í˜¸ì¶œ\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \": LangChainì˜ Products(ì œí’ˆ)ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”? ì˜ˆë¥¼ ë“¤ì–´ LangSmith, LangServe ê°™ì€ Productê°€ ìˆì–´\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed6c0a8",
   "metadata": {},
   "source": [
    "Runnableì˜ stream() í•¨ìˆ˜ í˜¸ì¶œ  \n",
    "\n",
    "ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•œ ìš”ì²­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa5746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ì›ë¦¬ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ì‚¬ëŒì€ ê²½í—˜ì„ í†µí•´ í•™ìŠµí•˜ê³ , ì¸ê³µì§€ëŠ¥ ëª¨ë¸ë„ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "**ì¸ê³µì§€ëŠ¥ ëª¨ë¸ í•™ìŠµ ê³¼ì •**\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ë¬¸ì œì— ëŒ€í•œ ë‹µì„ í¬í•¨í•˜ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
      "2. **ë°ì´í„° ì „ì²˜ë¦¬**: ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ê¹¨ë—í•˜ê²Œ ì •ë¦¬í•˜ê³ , í•„ìš”í•œ ê²½ìš° ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê±°ë‚˜ ê°€ê³µí•©ë‹ˆë‹¤.\n",
      "3. **ëª¨ë¸ ì´ˆê¸°í™”**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ë¬´ì‘ìœ„ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
      "4. **ìˆœì „íŒŒ**: ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ê³ , ì¶œë ¥ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì…ë ¥ ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
      "5. **ì˜¤ì°¨ ê³„ì‚°**: ì˜ˆì¸¡í•œ ì¶œë ¥ê°’ê³¼ ì‹¤ì œ ì¶œë ¥ê°’(ë°ì´í„°ì— í¬í•¨ëœ ë‹µ) ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
      "6. **ì—­ì „íŒŒ**: ì˜¤ì°¨ë¥¼ ì—­ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬, ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
      "7. **ìµœì í™”**: ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ì—¬, ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
      "\n",
      "**í•™ìŠµì˜ í•µì‹¬**\n",
      "\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµì€ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©°, ëª¨ë¸ì€ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "* **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**: ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ê°’ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ê°’ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë” ì˜ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
      "* **í¸í–¥ ì—…ë°ì´íŠ¸**: í¸í–¥ì€ ì¶œë ¥ê°’ì˜ í‰ê· ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í¸í–¥ì„ ì—…ë°ì´íŠ¸í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì€ ì¶œë ¥ê°’ì˜ í‰ê· ì„ ë” ì˜ ì˜ˆì¸¡í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "**í•™ìŠµì˜ ì¢…ë£Œ**\n",
      "\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµì€ ì˜¤ì°¨ê°€ ì¼ì • ìˆ˜ì¤€ ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ë•Œê¹Œì§€ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì˜¤ì°¨ê°€ ì¼ì • ìˆ˜ì¤€ ì´í•˜ë¡œ ë–¨ì–´ì§€ë©´, ëª¨ë¸ì€ í•™ìŠµì„ ì™„ë£Œí•œ ìƒíƒœì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ë ‡ê²Œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê³ , ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•œ ìš”ì²­\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"})\n",
    "    \n",
    "    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "    #print(answer)\n",
    "    for token in answer:\n",
    "        # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë°›ì€ ë°ì´í„°ì˜ ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥í•˜ê³ , ë²„í¼ë¥¼ ì¦‰ì‹œ ë¹„ì›ë‹ˆë‹¤.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10240d",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "**ì²«ë²ˆì§¸ Chainì˜ ì¶œë ¥ì´, ë‘ë²ˆì§¸ Chainì˜ ì…ë ¥ì´ ëœë‹¤.**\n",
    "**ë‘ê°œì˜ Chainê³¼ Prompt + OutputParserë¥¼ LCELë¡œ ì—°ê²°í•˜ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a42ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¥ë¥´ì— ë”°ë¼ ì˜í™” ì¶”ì²œ\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} ì¥ë¥´ì—ì„œ ì¶”ì²œí•  ë§Œí•œ ì˜í™”ë¥¼ í•œ í¸ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# Step 2: ì¶”ì²œëœ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ ìš”ì•½\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} ì¶”ì „í•œ ì˜í™”ì˜ ì œëª©ì„ ë¨¼ì € ì•Œë ¤ì£¼ì‹œê³ , ì¤„ì„ ë°”ê¾¸ì–´ì„œ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 1: ì˜í™” ì¶”ì²œ (ì…ë ¥: ì¥ë¥´ â†’ ì¶œë ¥: ì˜í™” ì œëª©)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f08e764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ì˜í™”: ë²”ì£„ë„ì‹œ2**\n",
      "\n",
      "ë²”ì£„ë„ì‹œ2ëŠ” ë§ˆë™ì„ì´ ì£¼ì—°ì„ ë§¡ì€ ëŒ€í•œë¯¼êµ­ì—ì„œ ê°€ì¥ ë§ì€ ê´€ê°ì„ ë™ì›í•œ ì˜í™” ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ì˜í™”ëŠ” ê°•ë ¥ë°˜ì´ ë² íŠ¸ë‚¨ìœ¼ë¡œ ì¶œì¥ì„ ê°€ì„œ ë§ˆì•½ ë²”ì£„ì¡°ì§ì„ ì†Œíƒ•í•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. ì˜í™”ëŠ” ë²”ì£„ë„ì‹œ1ì— ì´ì–´ ë”ìš± í™”ë ¤í•œ ì•¡ì…˜ê³¼ ìŠ¤í† ë¦¬ë¡œ ë§ì€ ê´€ê°ë“¤ì˜ ì‚¬ë‘ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸ 2: ì¤„ê±°ë¦¬ ìš”ì•½ (ì…ë ¥: ì˜í™” ì œëª© â†’ ì¶œë ¥: ì¤„ê±°ë¦¬)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1ì˜ ì¶œë ¥ì„ movie ì…ë ¥ ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # ì‹¤í–‰: \"SF\" ì¥ë¥´ì˜ ì˜í™” ì¶”ì²œ ë° ì¤„ê±°ë¦¬ ìš”ì•½\n",
    "    response = chain2.invoke({\"genre\": \"í•œêµ­ì˜í™”ì•¡ì…˜\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e818190",
   "metadata": {},
   "source": [
    "### PromptTemplate ì—¬ëŸ¬ê°œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "155c4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ChatGPTëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, ìˆ˜ì‹­ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì–¸ì–´ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ì´í•´í•˜ë©°, ì´ë¥¼ í†µí•´ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "ChatGPT ëª¨ë¸ì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìƒì„±: ChatGPTëŠ” ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "*   ì§€ì‹ ì •ë³´ ì œê³µ: ChatGPTëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë°©ëŒ€í•œ ì§€ì‹ì„ ê°€ì§€ê³  ìˆì–´ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "*   ì°½ì˜ì  ì½˜í…ì¸  ìƒì„±: ChatGPTëŠ” ì°½ì˜ì ì¸ ì½˜í…ì¸ , ì˜ˆë¥¼ ë“¤ì–´ ì‹œë‚˜ ì´ì•¼ê¸°, ì½”ë“œ, ì´ë©”ì¼, ë¬¸ì„œ, ë³´ê³ ì„œ, ë©”ì‹œì§€ ë“±ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ChatGPT ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ë‹¤ìŒê³¼ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   LLaMA\n",
      "*   PaLM\n",
      "*   BERT\n",
      "*   XLNet\n",
      "*   LaMDA\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# ë¬¸ìì—´ í…œí”Œë¦¿ ê²°í•© (PromptTemplate + PromptTemplate + ë¬¸ìì—´)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\")\n",
    "              + \"\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"ì˜ì–´\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"ì˜ì–´\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05f2dd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'Gemma ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'llama-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # ë¯¸ë¦¬ ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05efad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 ëª¨ë¸ì€ ëŒ€ê·œëª¨ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì–¸ì–´ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê°•í™” í•™ìŠµ ë° ì§€ë„ í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ë˜ì—ˆìœ¼ë©° ì¸í„°ë„·, ì±… ë° ëŒ€í™”ì—ì„œ ì–»ì€ ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.\n",
      "Gemma ëª¨ë¸ì€ ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ìê¸° ì§€ë„ í•™ìŠµ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì–¸ì–´ì˜ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤. í•™ìŠµëœ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ GemmaëŠ” ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "llama-4 ëª¨ë¸ì€ ë©”íƒ€ì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë˜ë©°, ì´ë¥¼ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ llama-4ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ë©°, ì´ ê³¼ì •ì„ í†µí•´ ì–¸ì–´ì˜ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ llama-4ëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) #AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664bd24",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate  \n",
    "\n",
    "SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate ì‚¬ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14da0191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ì¸ê³µì‹ ê²½ë§ì€ ì¸ê°„ì˜ ë‡Œë¥¼ ëª¨ë°©í•œ êµ¬ì¡°ë¡œ, ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  í•™ìŠµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì€ ì´ëŸ¬í•œ ì¸ê³µì‹ ê²½ë§ì„ ê¹Šê²Œ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n",
      "\n",
      "ì¦‰, ì—¬ëŸ¬ê°œì˜ ë ˆì´ì–´ë¥¼ ê°€ì§„ ì‹¬ì¸µ ì‹ ê²½ë§ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  í•™ìŠµí•˜ëŠ” ê¸°ìˆ ë¡œ, **ì»´í“¨í„°ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡** í•©ë‹ˆë‹¤. \n",
      "\n",
      "ì£¼ë¡œ ì´ë¯¸ì§€ë‚˜ ìŒì„±, ìì—°ì–´ ì²˜ë¦¬ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ì¸ì‹, ì–¼êµ´ ì¸ì‹, ìŒì„± ì¸ì‹, ì–¸ì–´ ë²ˆì—­, ììœ¨ ì£¼í–‰ ìë™ì°¨ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"ë‹¹ì‹ ì€ {topic} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ëª…í™•í•˜ê³  ìì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"ë”¥ëŸ¬ë‹ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb3e44",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "ì˜ˆì‹œë¥¼ ì œê³µ í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\"})\n",
    "#result = chain.invoke({\"input\": \"ì–‘ì ì–½í˜ì´ ë¬´ì—‡ì¸ê°€ìš”?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac7c6a",
   "metadata": {},
   "source": [
    "### PartialPromptTemplate\n",
    "* í”„ë¡¬í”„íŠ¸ì˜ ì…ë ¥ ê°’ì— í•¨ìˆ˜ í˜¸í’€ì´ë‚˜ ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•œ ë™ì ì¸ ê°’ì„ ëŒ€ì…í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì…ë‹ˆë‹¤.\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season(\"south\")}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")  # 'íƒœí’ ë°œìƒ'ì€ ì—¬ë¦„ê³¼ ê´€ë ¨ë¨\n",
    "result = model.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ”¹ í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79819baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# {info} ë³€ìˆ˜ì— APIì—ì„œ ë°›ì€ í™˜ìœ¨ ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ë°˜ì˜\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œ ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ëª¨ë¸ ì„¤ì • (GPT-4o-mini ì‚¬ìš©)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ”¹ í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-AGfTwH54-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
